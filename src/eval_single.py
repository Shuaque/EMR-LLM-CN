# dialogue_sample = [
#     "åŒ»ç”Ÿï¼šæœ€è¿‘å’³å—½å‰å®³å—ï¼Ÿç—°å¤šä¸å¤šï¼Ÿ",
#     "æ‚£è€…ï¼šå’³å—½æŒºæ˜æ˜¾çš„ï¼Œå°¤å…¶æ˜¯æ™šä¸Šï¼Œç—°æ˜¯ç™½è‰²çš„ã€‚",
#     "åŒ»ç”Ÿï¼šæœ‰æ²¡æœ‰å‘çƒ§æˆ–è€…èƒ¸ç—›ï¼Ÿ",
#     "æ‚£è€…ï¼šæ²¡æœ‰å‘çƒ§ï¼Œä½†æ˜¯æ·±å‘¼å¸çš„æ—¶å€™è‚‹éª¨è¿™è¾¹æœ‰ç‚¹ç–¼ã€‚",
#     "åŒ»ç”Ÿï¼šè¡Œï¼Œé‚£å…ˆå»æ‹ä¸ªCTæ£€æŸ¥ä¸€ä¸‹ï¼Œå†å¼€ç‚¹æ­¢å’³è¯ã€‚"
# ]

# dialogue_sample = [
#     "åŒ»ç”Ÿï¼šä½ å¹³æ—¶æŠ½ä¸æŠ½",
#     "æ‚£è€…ï¼šæˆ‘è¿˜å¥½ï¼Œä¹Ÿå°±å¶å°”æŠ½æŠ½çƒŸ",
#     "åŒ»ç”Ÿï¼šæœ‰æ²¡æœ‰å‘çƒ§æˆ–è€…èƒ¸ç—›ï¼Ÿ",
#     "æ‚£è€…ï¼šæ²¡æœ‰å‘çƒ§ï¼Œä½†æ˜¯æ·±å‘¼å¸çš„æ—¶å€™è‚‹éª¨è¿™è¾¹æœ‰ç‚¹ç–¼ã€‚",
#     "åŒ»ç”Ÿï¼šä½ å¹³æ—¶è¦å¤šæ³¨æ„èº«ä½“ï¼Œä¿æŠ¤è‚ºéƒ¨ï¼Œè¦ä¸ç„¶æœ‰ç™Œç—‡é£é™©ã€‚"
# ]
# # =================================================

import os
import torch
import argparse
import logging
import json
from fairseq import checkpoint_utils, utils

# Optimized for C3-MRAF | Symptom-Dominance Debugging | Logic Alignment [cite: 2025-12-28]

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("C3_MRAF_Inference")

# ================= Configuration =================
# ğŸŒŸ å»ºè®®æ›´æ–°ä¸ºæ‚¨çš„ 3B æœ€ä¼˜ Checkpoint
CHECKPOINT_PATH = "/workspace/shuaque/Classification_Semantic_att_LLM/exp/202512/run/28185426_A800_Optimized_Exp2_1_5B_3loss/checkpoints/checkpoint_best.pt"
USER_DIR = "/workspace/shuaque/Classification_Semantic_att_LLM/src_for_optim_subtopic"
ONTOLOGY_PATH = "/workspace/shuaque/Classification_Semantic_att_LLM/data/ontology.json"
dialogue_sample = [
    "åŒ»ç”Ÿï¼šæœ€è¿‘å’³å—½å‰å®³å—ï¼Ÿç—°å¤šä¸å¤šï¼Ÿ",
    "æ‚£è€…ï¼šå’³å—½æŒºæ˜æ˜¾çš„ï¼Œå°¤å…¶æ˜¯æ™šä¸Šï¼Œç—°æ˜¯ç™½è‰²çš„ã€‚",
    "åŒ»ç”Ÿï¼šæœ‰æ²¡æœ‰å‘çƒ§æˆ–è€…èƒ¸ç—›ï¼Ÿ",
    "æ‚£è€…ï¼šæ²¡æœ‰å‘çƒ§ï¼Œä½†æ˜¯æ·±å‘¼å¸çš„æ—¶å€™è‚‹éª¨è¿™è¾¹æœ‰ç‚¹ç–¼ã€‚",
    "åŒ»ç”Ÿï¼šè¡Œï¼Œé‚£å…ˆå»æ‹ä¸ªCTæ£€æŸ¥ä¸€ä¸‹ï¼Œå†å¼€ç‚¹æ­¢å’³è¯ã€‚"
]
# dialogue_sample = [
#     "åŒ»ç”Ÿï¼šä½ å¹³æ—¶æŠ½ä¸æŠ½",
#     "æ‚£è€…ï¼šæˆ‘è¿˜å¥½ï¼Œä¹Ÿå°±å¶å°”æŠ½æŠ½çƒŸ",
#     "åŒ»ç”Ÿï¼šæœ‰æ²¡æœ‰å‘çƒ§æˆ–è€…èƒ¸ç—›ï¼Ÿ",
#     "æ‚£è€…ï¼šæ²¡æœ‰å‘çƒ§ï¼Œä½†æ˜¯æ·±å‘¼å¸çš„æ—¶å€™è‚‹éª¨è¿™è¾¹æœ‰ç‚¹ç–¼ã€‚",
#     "åŒ»ç”Ÿï¼šä½ å¹³æ—¶è¦å¤šæ³¨æ„èº«ä½“ï¼Œä¿æŠ¤è‚ºéƒ¨ï¼Œè¦ä¸ç„¶æœ‰ç™Œç—‡é£é™©ã€‚"
# ]
# åŒ…å«æ˜æ˜¾â€œæ£€æŸ¥(Test)â€æŒ‡ä»¤çš„æ ·æœ¬
# dialogue_sample = [
#     "åŒ»ç”Ÿï¼šæœ€è¿‘å’³å—½å‰å®³å—ï¼Ÿç—°å¤šä¸å¤šï¼Ÿ",
#     "æ‚£è€…ï¼šå’³å—½æŒºæ˜æ˜¾çš„ï¼Œå°¤å…¶æ˜¯æ™šä¸Šï¼Œç—°æ˜¯ç™½è‰²çš„ã€‚",
#     "åŒ»ç”Ÿï¼šæœ‰æ²¡æœ‰å‘çƒ§æˆ–è€…èƒ¸ç—›ï¼Ÿ",
#     "æ‚£è€…ï¼šæ²¡æœ‰å‘çƒ§ï¼Œä½†æ˜¯æ·±å‘¼å¸çš„æ—¶å€™è‚‹éª¨è¿™è¾¹æœ‰ç‚¹ç–¼ã€‚",
#     "åŒ»ç”Ÿï¼šè¡Œï¼Œé‚£å…ˆå»æ‹ä¸ªæ‹ä¸ªCTæ£€æŸ¥ä¸€ä¸‹ï¼Œå†å¼€ç‚¹æ­¢å’³è¯ã€‚"
# ]

# ğŸŒŸ è®¾ç½®è®¡ç®—é…é¢ï¼š0.1-0.2 æ˜¯è§‚å¯Ÿ E-IAQ æ•ˆç‡çš„æœ€ä½³åŒºé—´
TEST_RATIO = 0.2
# =================================================

def build_prompt(utterances):
    context = "\n".join([u.strip() for u in utterances if u.strip()])
    return (
        "<|im_start|>system\n"
        "ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—ä¸“å®¶åŠ©æ‰‹ã€‚è¯·åˆ†æå¯¹è¯å¹¶æå–ï¼š1.ç—‡çŠ¶, 2.æ£€æŸ¥, 3.æ‰‹æœ¯, 4.ä¸€èˆ¬ä¿¡æ¯ã€‚<|im_end|>\n"
        "<|im_start|>user\n"
        f"å¯¹è¯å†…å®¹ï¼š\n{context}\n<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

def main():
    if USER_DIR:
        utils.import_user_module(argparse.Namespace(user_dir=USER_DIR))
    
    logger.info(f"Loading C3-MRAF Model from {CHECKPOINT_PATH}...")
    models, cfg, task = checkpoint_utils.load_model_ensemble_and_task(
        [CHECKPOINT_PATH],
        arg_overrides={'distributed_training': {'distributed_world_size': 1}}
    )
    model = models[0].cuda().eval().to(dtype=torch.bfloat16)
    
    # 1. å‡†å¤‡æ˜ å°„ä¸å±‚çº§æ©ç 
    with open(ONTOLOGY_PATH, "r", encoding="utf-8") as f:
        ontology = json.load(f)
    id2label = {int(v): k for k, v in ontology["label2id"].items()}
    topic_names = ["Symptom", "Test", "Surgery", "GeneralInfo"]
    
    # è·å–æ¨¡å‹çš„å±‚çº§æ©ç ç”¨äºè¯Šæ–­å±è”½é€»è¾‘
    m_inner = model.module if hasattr(model, 'module') else model
    h_mask = m_inner.hierarchical_mask.float().cpu() # [4, 206]

    # 2. è¾“å…¥å¤„ç†
    full_prompt = build_prompt(dialogue_sample)
    tokenized = task.tokenizer(full_prompt, max_length=512, truncation=True, return_tensors="pt")
    input_ids, attention_mask = tokenized["input_ids"].cuda(), tokenized["attention_mask"].cuda()

    # 3. æ¨ç†
    logger.info(f"Inference Running (Ratio: {TEST_RATIO})...")
    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):
        # ä¼ é€’ ratio è§¦å‘ E-IAQ åŠ¨æ€åé¢åˆ†é… [cite: 2025-12-28]
        output = model(input_ids=input_ids, attention_mask=attention_mask, ratio=TEST_RATIO)
    
    probs_topic = torch.sigmoid(output["logits_topic"].squeeze(0).float())
    # æ³¨æ„ï¼šlogits_subtopic å·²ç»è¿‡ model.py ä¸­çš„ Log-masking Shield å¤„ç† [cite: 2025-12-28]
    probs_sub = torch.sigmoid(output["logits_subtopic"].squeeze(0).float())

    # 4. æ ¼å¼åŒ–è¾“å‡º
    print("\n" + "="*75)
    print(f" ğŸ¥ Medical Entity Recognition - C3-MRAF (Diagnostic Mode)")
    print("="*75)
    
    # --- ä¸»ç±»è¯é¢˜ç»“æœ ---
    print(f"{'Prob':<8} | {'Topic Category':<15} | {'Status'}")
    print("-" * 45)
    for i, p in enumerate(probs_topic.tolist()):
        status = "âœ… ACTIVE" if p > 0.5 else "âŒ MASKED"
        print(f"{p:.4f}   | {topic_names[i]:<15} | {status}")

    # --- å­ç±»å®ä½“ç»“æœ ---
    print("\n" + f"{'Prob':<8} | {'Sub-category (Entities)':<30} | {'Logic Gate'}")
    print("-" * 75)
    
    # æå– Top-10 æ¦‚ç‡
    top_v, top_i = torch.topk(probs_sub, k=10)
    for p, idx in zip(top_v.tolist(), top_i.tolist()):
        label = id2label.get(idx, "Unknown")
        
        # è¯Šæ–­ï¼šå¯»æ‰¾è¯¥å­ç±»æ‰€å±çš„ä¸»ç±»æ¦‚ç‡
        parent_topic_idx = torch.where(h_mask[:, idx] > 0.5)[0][0].item()
        parent_prob = probs_topic[parent_topic_idx].item()
        
        gate_info = "ğŸŸ¢ Pass" if parent_prob > 0.5 else f"ğŸ”´ Blocked by {topic_names[parent_topic_idx]}({parent_prob:.2f})"
        
        marker = "âœ…" if p > 0.5 else "  "
        print(f"{p:.4f}   | {marker} {label:<30} | {gate_info}")

    # --- æ•ˆç‡æŒ‡æ ‡ ---
    selected_num = len(output["selected_indices"][0])
    print("\n" + "-" * 75)
    print(f"Efficiency: E-IAQ verified {selected_num}/206 entities ({selected_num/206*100:.1f}% budget used)")
    print("="*75 + "\n")

if __name__ == "__main__":
    main()