# EMR-LLM-CN
An audio-visual LLM



### Environment Setup
```bash

conda create -n emr python=3.9 -y
conda activate emr
git clone https://github.com/Shuaque/EMR-LLM-CN.git
cd EMR-LLM-CN

# PyTorch and related packages
pip install -U "pip<24.1" "setuptools<72" #(If your pip version > 24.1, please run this)
pip install "PyYAML>=5.1" "omegaconf==2.0.6" "hydra-core==1.0.7"


cd fairseq
pip install --editable ./

pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121
pip install numpy==1.23.5 scipy opencv-python
pip install editdistance python_speech_features einops soundfile sentencepiece tqdm tensorboard unidecode librosa
pip install omegaconf==2.0.6 hydra-core==1.0.7 #(If your pip version > 24.1, please run "python3 -m pip install --upgrade pip==24.0")
pip install transformers==4.47.1 peft==0.14.0
pip install bitsandbytes --prefer-binary
pip install tensorboardX 
pip install av matplotlib

```




### Download 
1. Download Multimodal(audio-visual)EMR dataset generated by AI below. 
| File Name              | Source URL                                                                              | File Size  |
|------------------------|-----------------------------------------------------------------------------------------|------------|
| LRS3_landmarks.zip     |[GoogleDrive](https://bit.ly/33rEsax) or [BaiduDrive](https://bit.ly/3rwQSph)(key: mi3c) |     18GB   |
| LRS2_landmarks.zip     |[GoogleDrive](https://bit.ly/3jSMMoz) or [BaiduDrive](https://bit.ly/3BuIwBB)(key: 53rc) |     9GB    |


2. Download pre-computed landmarks below. Once you've finished downloading the five files, simply merge them into one single file using `zip -FF vox2_landmarks.zip --out single.zip`, and then decompress it. If you leave `landmarks-dir` empty, landmarks will be provided with the used of `detector`.

| File Name              | Source URL                                                                        | File Size |
|------------------------|-----------------------------------------------------------------------------------|-----------|
| vox2_landmarks.zip     | [Download](https://www.doc.ic.ac.uk/~pm4115/vox2landmarks/vox2_landmarks.zip)     | 18GB      |
| vox2_landmarks.z01     | [Download](https://www.doc.ic.ac.uk/~pm4115/vox2landmarks/vox2_landmarks.z01)     | 20GB      |
| vox2_landmarks.z02     | [Download](https://www.doc.ic.ac.uk/~pm4115/vox2landmarks/vox2_landmarks.z02)     | 20GB      |
| vox2_landmarks.z03     | [Download](https://www.doc.ic.ac.uk/~pm4115/vox2landmarks/vox2_landmarks.z03)     | 20GB      |
| vox2_landmarks.z04     | [Download](https://www.doc.ic.ac.uk/~pm4115/vox2landmarks/vox2_landmarks.z04)     | 20GB      |

## Model zoo

<details open>

<summary>LRS3</summary>

<p> </p>

| Model                                 | Training data (h)  |  WER [%]   |  Params (M) |    MD5            |
|---------------------------------------|:------------------:|:----------:|:-----------:|:------------------------:|
| [`vsr_trlrs3_23h_base.pth`](https://drive.google.com/file/d/1FDDTOBteJV8yBiJ8yePtZ-C-xR4s80rV/view?usp=sharing)             |        438          |    93.0    |     250     | fc8db  |
| [`vsr_trlrs3_base.pth`](https://drive.google.com/file/d/12PNM5szUsk_CuaV1yB9dL_YWvSM1zvAd/view?usp=sharing)                 |        438          |    36.0    |     250     | c00a7  |
| [`vsr_trlrs3vox2_base.pth`](https://drive.google.com/file/d/1shcWXUK2iauRhW9NbwCc25FjU1CoMm8i/view?usp=sharing)             |        1759         |    24.6    |     250     | 774a6  |
| [`vsr_trlrs2lrs3vox2avsp_base.pth`](https://drive.google.com/file/d/1r1kx7l9sWnDOCnaFHIGvOtzuhFyFA88_/view?usp=sharing)     |        3291         |    20.3    |     250     | 49f77  |
| [`asr_trlrs3_base.pth`](https://drive.google.com/file/d/1IBMkI7XyZo8mF3rz109rXrMH7MyxRuiY/view?usp=sharing)                 |        438          |    2.0     |     243     | 8af72  |
| [`asr_trlrs3vox2_base.pth`](https://drive.google.com/file/d/1YN9lwZN6iWn2qNQRpfpGpnf2r6ZTQqVT/view?usp=sharing)             |        1759         |    1.0     |     243     | f0c5c  |

Some results are slightly better than in the paper due to hyper-parameter optimisation. The av-asr code and checkpoint can be found on the released version.

</details>
